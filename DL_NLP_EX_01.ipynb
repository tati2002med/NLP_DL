{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercice I"
      ],
      "metadata": {
        "id": "VTmu2PNI7Rng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Réaliser un correcteur d'orthographe en python pour la langue anglaise en se basant sur l'algorithme implémenté dans les TPs."
      ],
      "metadata": {
        "id": "HskaOEhm7YZB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g7w0qllMPi4Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0TomdCMPtFq",
        "outputId": "a494d233-83a2-4363-eeb1-dbed96dce4cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-iBcSKTPj_g",
        "outputId": "76ed7b6e-338f-4b8b-ac17-3e775a2a49b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'fulton', 'county', 'grand', 'jury']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "vocab = list(brown.words())\n",
        "vocab = [v.lower() for v in vocab]\n",
        "vocab[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "QqIYqePWqNih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIazofJiPqjg",
        "outputId": "7428d6b6-5879-4342-eb29-c819798c0d4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "collections.Counter"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "word_count_dict = Counter(vocab)\n",
        "type(word_count_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J7Qc-YQlP17V"
      },
      "outputs": [],
      "source": [
        "def get_probs(word_count_dict):\n",
        "    '''\n",
        "    Input:\n",
        "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
        "    Output:\n",
        "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
        "    '''\n",
        "    probs = {}  # return this variable correctly\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    m = sum(word_count_dict.values())\n",
        "    for key in word_count_dict.keys():\n",
        "        probs[key] = word_count_dict[key] / m\n",
        "    ### END CODE HERE ###\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0CRE9U3QCYy",
        "outputId": "31c7916a-b893-455f-d010-0db87f600de1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "probs = get_probs(word_count_dict)\n",
        "type(probs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SHsh_A5uQEoZ"
      },
      "outputs": [],
      "source": [
        "def generate_candidates_by_one_edit(word):\n",
        "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    \n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [left + right[1:] for left, right in splits if right]\n",
        "    switchs = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right) > 1]\n",
        "    replaces = [left + c + right[1:] for left, right in splits if right for c in alphabet]\n",
        "    inserts = [left + c + right for left, right in splits for c in alphabet]\n",
        "\n",
        "    return set(deletes + switchs + replaces + inserts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0C4w7KCQW9q",
        "outputId": "e6d9c1da-1544-47c6-9055-fdf3a20c7314"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "cand = generate_candidates_by_one_edit('sould')\n",
        "type(cand)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0WzN4i0JQYxm"
      },
      "outputs": [],
      "source": [
        "def get_candidate_by_two_edit(word):\n",
        "    edit_two_set = set()\n",
        "    edit_one = generate_candidates_by_one_edit(word)\n",
        "    for dandidates in edit_one:\n",
        "        edit_two = generate_candidates_by_one_edit(dandidates)\n",
        "        edit_two_set.update(edit_two)\n",
        "    return edit_two_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JygD6KzxQbGe",
        "outputId": "c617e25f-2892-48df-d90d-40fb87886ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36860\n"
          ]
        }
      ],
      "source": [
        "edited_twice = get_candidate_by_two_edit('sould')\n",
        "print(len(edited_twice))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Cp95kaDfQcwb"
      },
      "outputs": [],
      "source": [
        "def get_candidate_by_three_edit(word):\n",
        "  edit_three_set = set()\n",
        "  edit_two = get_candidate_by_two_edit(word)\n",
        "  for candidates in edit_two:\n",
        "    edit_three = generate_candidates_by_one_edit(candidates)\n",
        "    edit_three_set.update(edit_three)\n",
        "  return edit_three_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3LPTEej9Q5_D"
      },
      "outputs": [],
      "source": [
        "def get_valid_corrections(word,vocab):\n",
        "  # candidates = get_candidate_by_two_edit(word)\n",
        "  # valid_candidates=[]\n",
        "  suggestions = list((word in vocab and word) or \n",
        "                     generate_candidates_by_one_edit(word).intersection(vocab) or \n",
        "                     get_candidate_by_two_edit(word).intersection(vocab) or \n",
        "                     get_candidate_by_three_edit(word).intersection(vocab))\n",
        "  # for cand in candidates:\n",
        "  #   if cand in vocab:\n",
        "  #     valid_candidates.append(cand)\n",
        "  # valid_candidates = [candidate for candidate in candidates if candidate in vocab]\n",
        "  \n",
        "  return suggestions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORaGWVVVbhI2",
        "outputId": "1cf707c2-2554-4ead-c1f0-47928e80860f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gould',\n",
              " 'soul',\n",
              " 'mould',\n",
              " 'sold',\n",
              " 'sound',\n",
              " 'should',\n",
              " 'could',\n",
              " 'ould',\n",
              " 'would',\n",
              " 'shuld',\n",
              " 'souls',\n",
              " 'soule']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "get_valid_corrections('sould',vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "NvyVY9nMKfpK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OCyMDsuJbk1i"
      },
      "outputs": [],
      "source": [
        "def q1_solution(text,vocab):\n",
        "  words = re.findall(r'\\w+', text.lower())\n",
        "  corrected_text=''\n",
        "  for word in words:\n",
        "    if word not in vocab :\n",
        "      corrected_word = get_valid_corrections(word, vocab)\n",
        "      if len(corrected_word) == 1:\n",
        "        print(\"miss spelled word detected (\"+word+\") the suggested corrections is :\"+ str(corrected_word))\n",
        "      else:\n",
        "        print(\"miss spelled word detected (\"+word+\") the suggested corrections are :\"+ str(corrected_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer the question 1.1"
      ],
      "metadata": {
        "id": "77WzFHlU-Mrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aYZB31KYdYBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e8dd9d9-ac83-4381-dbc3-252384f7f347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "miss spelled word detected (writinf) the suggested corrections is :['writing']\n",
            "miss spelled word detected (texr) the suggested corrections are :['tex', 'text', 'tear']\n",
            "miss spelled word detected (edito) the suggested corrections are :['edith', 'edit', 'editor']\n"
          ]
        }
      ],
      "source": [
        "q1_solution('I enjoy writinf texr edito',vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementer une variante du programme precedent qui utilise un modele de langue pour ordonner les corrections possibles par leurs probabilites selon le modele de langue et selon la distance minimale d'edition.\n"
      ],
      "metadata": {
        "id": "sce0U9S075K5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Min Edit Distance"
      ],
      "metadata": {
        "id": "DClCpTS5CciL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_edit_distance(source, target, ins_cost=1, del_cost=1, rep_cost=2):\n",
        "    '''\n",
        "    Input:\n",
        "        source: a string corresponding to the string you are starting with\n",
        "        target: a string corresponding to the string you want to end with\n",
        "        ins_cost: an integer setting the insert cost\n",
        "        del_cost: an integer setting the delete cost\n",
        "        rep_cost: an integer setting the replace cost\n",
        "    Output:\n",
        "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
        "        med: the minimum edit distance (med) required to convert the source string to the target\n",
        "    '''\n",
        "    # use deletion and insert cost as  1\n",
        "    m = len(source)\n",
        "    n = len(target)\n",
        "    # initialize cost matrix with zeros and dimensions (m+1,n+1)\n",
        "    D = np.zeros((m + 1, n + 1), dtype=int)\n",
        "\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "\n",
        "    # Fill in column 0, from row 1 to row m, both inclusive\n",
        "    for row in range(1, m + 1):  # Replace None with the proper range\n",
        "        D[row, 0] = D[row - 1, 0] + del_cost\n",
        "\n",
        "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
        "    for col in range(1, n + 1):  # Replace None with the proper range\n",
        "        D[0, col] = D[0, col - 1] + ins_cost\n",
        "\n",
        "    # Loop through row 1 to row m, both inclusive\n",
        "    for row in range(1, m + 1):\n",
        "\n",
        "        # Loop through column 1 to column n, both inclusive\n",
        "        for col in range(1, n + 1):\n",
        "\n",
        "            # Intialize r_cost to the 'replace' cost that is passed into this function\n",
        "            r_cost = rep_cost\n",
        "\n",
        "            # Check to see if source character at the previous row\n",
        "            # matches the target character at the previous column,\n",
        "            if source[row - 1] == target[col - 1]:\n",
        "                # Update the replacement cost to 0 if source and target are the same\n",
        "                r_cost = 0\n",
        "\n",
        "            # Update the cost at row, col based on previous entries in the cost matrix\n",
        "            # Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)\n",
        "            D[row, col] = min([D[row - 1, col] + del_cost, D[row, col - 1] + ins_cost, D[row - 1, col - 1] + r_cost])\n",
        "\n",
        "    # Set the minimum edit distance with the cost found at row m, column n\n",
        "    med = D[m, n]\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return med"
      ],
      "metadata": {
        "id": "IoqBSUfV7-Si"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q2_solution_MED(text,vocab):\n",
        "  words = re.findall(r'\\w+', text.lower())\n",
        "  corrected_text=''\n",
        "  for word in words:\n",
        "    if word not in vocab :\n",
        "      corrected_word = get_valid_corrections(word, vocab)\n",
        "      if len(corrected_word) == 1:\n",
        "        print(\"miss spelled word detected (\"+word+\") the suggested corrections is :\"+ str(corrected_word))\n",
        "      else:\n",
        "        print(\"miss spelled word detected (\"+word+\") the suggested corrections is : \"+ str(corrected_word))\n",
        "        sorted_corrections=sorted(corrected_word,key=lambda x:min_edit_distance(x,word))\n",
        "        for sor_corr in sorted_corrections:\n",
        "          print(\"miss spelled word detected (\"+word+\") the suggested corrections sorted by MED are :\"+ str(sor_corr)+\" min edit distance = \"+str(min_edit_distance(sor_corr,word)))\n"
      ],
      "metadata": {
        "id": "ATfkixdPDFy2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2_solution_MED('I enjoy writinf texr edito',vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciIJ5Pp6EtwD",
        "outputId": "8685d4b0-9772-475c-96b5-27be0bb40523"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "miss spelled word detected (writinf) the suggested corrections is :['writing']\n",
            "miss spelled word detected (texr) the suggested corrections is : ['tex', 'text', 'tear']\n",
            "miss spelled word detected (texr) the suggested corrections sorted by MED are :tex min edit distance = 1\n",
            "miss spelled word detected (texr) the suggested corrections sorted by MED are :text min edit distance = 2\n",
            "miss spelled word detected (texr) the suggested corrections sorted by MED are :tear min edit distance = 2\n",
            "miss spelled word detected (edito) the suggested corrections is : ['edith', 'edit', 'editor']\n",
            "miss spelled word detected (edito) the suggested corrections sorted by MED are :edit min edit distance = 1\n",
            "miss spelled word detected (edito) the suggested corrections sorted by MED are :editor min edit distance = 1\n",
            "miss spelled word detected (edito) the suggested corrections sorted by MED are :edith min edit distance = 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2_solution_MED('dhis is a mass speld wrd',vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_Gnsj4RE22e",
        "outputId": "592bf886-adcb-4245-b694-0aa9c66f28c8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "miss spelled word detected (dhis) the suggested corrections is : ['this', 'dais', 'phis', 'his']\n",
            "miss spelled word detected (dhis) the suggested corrections sorted by MED are :his min edit distance = 1\n",
            "miss spelled word detected (dhis) the suggested corrections sorted by MED are :this min edit distance = 2\n",
            "miss spelled word detected (dhis) the suggested corrections sorted by MED are :dais min edit distance = 2\n",
            "miss spelled word detected (dhis) the suggested corrections sorted by MED are :phis min edit distance = 2\n",
            "miss spelled word detected (speld) the suggested corrections is : ['spend', 'spell', 'speed', 'sped']\n",
            "miss spelled word detected (speld) the suggested corrections sorted by MED are :sped min edit distance = 1\n",
            "miss spelled word detected (speld) the suggested corrections sorted by MED are :spend min edit distance = 2\n",
            "miss spelled word detected (speld) the suggested corrections sorted by MED are :spell min edit distance = 2\n",
            "miss spelled word detected (speld) the suggested corrections sorted by MED are :speed min edit distance = 2\n",
            "miss spelled word detected (wrd) the suggested corrections is : ['ward', 'word', 'wed', 'wry', 'wod']\n",
            "miss spelled word detected (wrd) the suggested corrections sorted by MED are :ward min edit distance = 1\n",
            "miss spelled word detected (wrd) the suggested corrections sorted by MED are :word min edit distance = 1\n",
            "miss spelled word detected (wrd) the suggested corrections sorted by MED are :wed min edit distance = 2\n",
            "miss spelled word detected (wrd) the suggested corrections sorted by MED are :wry min edit distance = 2\n",
            "miss spelled word detected (wrd) the suggested corrections sorted by MED are :wod min edit distance = 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Language Model"
      ],
      "metadata": {
        "id": "IkxuyMB5GXfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngrams(text, n):\n",
        "    # Convert the text to lowercase and remove punctuation\n",
        "    text = text.lower()\n",
        "    text = text.replace(\".\", \"\").replace(\",\", \"\").replace(\"!\", \"\").replace(\"?\", \"\")\n",
        "    \n",
        "    # Tokenize the text into individual words\n",
        "    words = text.split()\n",
        "    \n",
        "    # Generate n-grams\n",
        "    n_grams = []\n",
        "    for i in range(len(words) - n + 1):\n",
        "        n_gram = (words[i:i+n])\n",
        "        n_grams.append(tuple(n_gram))\n",
        "    \n",
        "    return n_grams\n",
        "\n",
        "# Example usage\n",
        "sentence = \"This is an example sentence.\"\n",
        "result = generate_ngrams(sentence,2)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0jwbp32Ggi9",
        "outputId": "c62091b7-098b-4482-b69e-bb34124c10b6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('this', 'is'), ('is', 'an'), ('an', 'example'), ('example', 'sentence')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "fD8bnbl2JCX1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = brown.sents()"
      ],
      "metadata": {
        "id": "vf2_ZhHHK1zH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
        "    \"\"\"\n",
        "    Count all n-grams in the data\n",
        "    \n",
        "    Args:\n",
        "        data: List of lists of words\n",
        "        n: number of words in a sequence\n",
        "    \n",
        "    Returns:\n",
        "        A dictionary that maps a tuple of n-words to its frequency\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize dictionary of n-grams and their counts\n",
        "    n_grams = {}\n",
        "\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    \n",
        "    # Go through each sentence in the data\n",
        "    for sentence in data: # complete this line\n",
        "        \n",
        "        # prepend start token n times, and  append <e> one time\n",
        "        sentence = [s.lower() for s in sentence]\n",
        "        sentence = ([start_token] * n)+ sentence + [end_token]\n",
        "        \n",
        "        # convert list to tuple\n",
        "        # So that the sequence of words can be used as\n",
        "        # a key in the dictionary\n",
        "        sentence = tuple(sentence)\n",
        "        \n",
        "        # Use 'i' to indicate the start of the n-gram\n",
        "        # from index 0\n",
        "        # to the last index where the end of the n-gram\n",
        "        # is within the sentence.\n",
        "        m = len(sentence) if n==1 else len(sentence)-1\n",
        "        for i in range(m): # complete this line\n",
        "\n",
        "            # Get the n-gram from i to i+n\n",
        "            n_gram = sentence[i:i+n]\n",
        "\n",
        "            # check if the n-gram is in the dictionary\n",
        "            if n_gram in n_grams.keys(): # complete this line\n",
        "            \n",
        "                # Increment the count for this n-gram\n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                # Initialize this n-gram count to 1\n",
        "                n_grams[n_gram] = 1\n",
        "    \n",
        "            ### END CODE HERE ###\n",
        "    return n_grams"
      ],
      "metadata": {
        "id": "LaQzR67aNuIn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bi_grams = count_n_grams(sentences,2)\n",
        "uni_grams = count_n_grams(sentences,1)"
      ],
      "metadata": {
        "id": "_dgE1-RFN1OS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probability(word, previous_n_gram, uni_grams, bi_grams, vocabulary_size, k=1.0):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
        "    \n",
        "    Args:\n",
        "        word: next word\n",
        "        previous_n_gram: A sequence of words of length n\n",
        "        n_gram_counts: Dictionary of counts of n-grams\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        vocabulary_size: number of words in the vocabulary\n",
        "        k: positive constant, smoothing parameter\n",
        "    \n",
        "    Returns:\n",
        "        A probability\n",
        "    \"\"\"\n",
        "    denominator = k*vocabulary_size\n",
        "    numerator = k\n",
        "    if (word,) in uni_grams:\n",
        "      denominator += uni_grams[(word,)]\n",
        "    if(previous_n_gram,word) in bi_grams:\n",
        "      numerator += bi_grams[(previous_n_gram,word)]\n",
        "\n",
        "    probability = numerator/denominator\n",
        "    # ### END CODE HERE ###\n",
        "    \n",
        "    return np.log(probability)\n"
      ],
      "metadata": {
        "id": "ejyFHDI6K9gx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(set(vocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Pcp6t3ne2mL",
        "outputId": "58ad1b4d-b014-4651-c85a-5412c0c58623"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49815"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(uni_grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljR4Lwjwe5xT",
        "outputId": "26d39ef3-3f77-4568-fc33-101073feadfb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49817"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimate_probability('am','i',uni_grams,bi_grams,len(list(set(vocab))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgyfwgoTitiE",
        "outputId": "22999b25-dd1c-4831-be10-66eb3d63e18c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-5.483279664283628"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimate_probability('hard','very',uni_grams,bi_grams,len(list(set(vocab))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbU0Dvc7fkib",
        "outputId": "5459d21a-c818-4042-d9c8-6f7552af2e56"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-9.721505937955271"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimate_probability('afa','rr',uni_grams,bi_grams,len(list(set(vocab))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F_U8KtdhIal",
        "outputId": "2188fab9-d85f-4edf-96ec-b7f8cc87b26d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-10.816071422478958"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimate_probability('granted','for',uni_grams,bi_grams,len(list(set(vocab))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoHIHPerxhZU",
        "outputId": "98a29c5f-5cce-44ef-f558-d5f7fe381bf5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-8.419299677676308"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1/len(list(set(vocab)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd41Rd6_LcSN",
        "outputId": "60f98db1-fcb5-4e17-eced-3ac715cbb1ae"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.007427481682224e-05"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def q2_solution_PorbLangModel(text,vocab):\n",
        "  words = re.findall(r'\\w+', text.lower())\n",
        "  corrected_text=''\n",
        "  for word in words:\n",
        "    if word not in vocab :\n",
        "      corrected_word = get_valid_corrections(word, vocab)\n",
        "      if len(corrected_word) == 1:\n",
        "        print(\"miss spelled word detected (\"+word+\") the suggested corrections is :\"+ str(corrected_word))\n",
        "      else:\n",
        "        print(\"miss spelled word detected (\"+word+\") the suggested corrections are :\"+ str(corrected_word))\n",
        "        sorted_corrections=sorted(corrected_word,key=lambda x:estimate_probability(x,word,uni_grams, bi_grams, len(vocab)),reverse=True)\n",
        "        for sor_corr in sorted_corrections:\n",
        "          print(\"miss spelled word detected (\"+word+\") the suggested corrections sorted by probability is :\"+ str(sor_corr)+\" probability = \"+str(estimate_probability(sor_corr,word,uni_grams, bi_grams, len(vocab))))"
      ],
      "metadata": {
        "id": "LhMS8c8RMWTS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2_solution_PorbLangModel('I enjoy writinf texr edito',vocab)"
      ],
      "metadata": {
        "id": "ISJchhW-MpFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2763f76-3e6f-43bd-8d99-0589f61e1853"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "miss spelled word detected (writinf) the suggested corrections is :['writing']\n",
            "miss spelled word detected (texr) the suggested corrections are :['tex', 'text', 'tear']\n",
            "miss spelled word detected (texr) the suggested corrections sorted by probability is :tex probability = -13.9649584828678\n",
            "miss spelled word detected (texr) the suggested corrections sorted by probability is :tear probability = -13.96496709466355\n",
            "miss spelled word detected (texr) the suggested corrections sorted by probability is :text probability = -13.965009291390725\n",
            "miss spelled word detected (edito) the suggested corrections are :['edith', 'edit', 'editor']\n",
            "miss spelled word detected (edito) the suggested corrections sorted by probability is :edit probability = -13.964959344050714\n",
            "miss spelled word detected (edito) the suggested corrections sorted by probability is :edith probability = -13.964961066414313\n",
            "miss spelled word detected (edito) the suggested corrections sorted by probability is :editor probability = -13.96502393065556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2_solution_PorbLangModel('it is a bad habbitr to take thinds for grantef',vocab)"
      ],
      "metadata": {
        "id": "TIS14LssNr0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc09bf6a-6270-4490-a5d1-7370fc17448c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "miss spelled word detected (habbitr) the suggested corrections are :['habits', 'rabbits', 'babbitt', 'rabbit', 'habit']\n",
            "miss spelled word detected (habbitr) the suggested corrections sorted by probability is :babbitt probability = -13.964959344050714\n",
            "miss spelled word detected (habbitr) the suggested corrections sorted by probability is :rabbits probability = -13.964961927594999\n",
            "miss spelled word detected (habbitr) the suggested corrections sorted by probability is :rabbit probability = -13.96496709466355\n",
            "miss spelled word detected (habbitr) the suggested corrections sorted by probability is :habits probability = -13.964975706385136\n",
            "miss spelled word detected (habbitr) the suggested corrections sorted by probability is :habit probability = -13.964977428720553\n",
            "miss spelled word detected (thinds) the suggested corrections are :['thirds', 'things', 'thinks']\n",
            "miss spelled word detected (thinds) the suggested corrections sorted by probability is :thirds probability = -13.964961066414313\n",
            "miss spelled word detected (thinds) the suggested corrections sorted by probability is :thinks probability = -13.964977428720553\n",
            "miss spelled word detected (thinds) the suggested corrections sorted by probability is :things probability = -13.965274487198053\n",
            "miss spelled word detected (grantef) the suggested corrections is :['granted']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}