{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "beb24a5e",
      "metadata": {
        "id": "beb24a5e"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e464a7",
      "metadata": {
        "id": "41e464a7"
      },
      "source": [
        "Les données de test:(passage de format Conllu en format text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1a46d385",
      "metadata": {
        "id": "1a46d385"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_file = 'test.conllu'\n",
        "test_file = 'test_pos.txt'\n",
        "\n",
        "with open(input_file, 'r', encoding='utf-8') as input_f, open(test_file, 'w', encoding='utf-8') as output_f:\n",
        "    for line in input_f:\n",
        "        line = line.strip()\n",
        "        if line and not line.startswith('#'):\n",
        "            columns = line.split('\\t')\n",
        "            word = columns[1]  # L'indice 1 correspond à la colonne FORM (mot)\n",
        "            pos = columns[3]  # L'indice 3 correspond à la colonne UPOS (partie du discours universelle)\n",
        "            output_f.write(f\"{word}\\t{pos}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a90298aa",
      "metadata": {
        "id": "a90298aa"
      },
      "source": [
        "Les données de train:(passage de format Conllu en format text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6e73e3d6",
      "metadata": {
        "id": "6e73e3d6"
      },
      "outputs": [],
      "source": [
        "input_file = 'train.conllu'\n",
        "train_file = 'train_pos.txt'\n",
        "\n",
        "with open(input_file, 'r', encoding='utf-8') as input_f, open(train_file, 'w', encoding='utf-8') as output_f:\n",
        "    for line in input_f:\n",
        "        line = line.strip()\n",
        "        if line and not line.startswith('#'):\n",
        "            columns = line.split('\\t')\n",
        "            word = columns[1]  # L'indice 1 correspond à la colonne FORM (mot)\n",
        "            pos = columns[3]  # L'indice 3 correspond à la colonne UPOS (partie du discours universelle)\n",
        "            output_f.write(f\"{word}\\t{pos}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ff805a9",
      "metadata": {
        "id": "8ff805a9"
      },
      "source": [
        "Construction de vocabulaire a partir des données de train :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2f9d7975",
      "metadata": {
        "id": "2f9d7975"
      },
      "outputs": [],
      "source": [
        "voc_l = set()\n",
        "\n",
        "with open('train_pos.txt', 'r', encoding='utf-8') as input_f:\n",
        "    for line in input_f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            word, pos = line.split('\\t')\n",
        "            voc_l.add(word)\n",
        "voc_l=list(voc_l)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48a8e0a5",
      "metadata": {
        "id": "48a8e0a5"
      },
      "source": [
        "Affichage de qlq exemples de vocabulaire "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aeb272a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeb272a7",
        "outputId": "bbfd81c3-ffcb-4610-9b18-581bce3ba8c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulaire:\n",
            "A few items of the vocabulary list\n",
            "['ascendant', 'campagne', 'Network', 'boucher', 'osseux', 'Philosophy', 'ḏāl', 'littéraires', 'tropicales', 'attributions', 'Albéric', 'Bannoncourt', 'Classée', 'Camposanto', 'bourguignon', 'Sahraouie', 'arriveront', 'Commissaire', 'Opterre', 'Perpétuel', 'Gordes', 'hêmisus', 'lâché', 'Carahes', 'cristallin', 'novosti', 'Meierfrankenfeld', 'acceptable', 'XML', '+0,6', 'écoulant', 'destructeurs', 'Vanuatu', 'continuent', 'authentiquement', 'retracée', 'impeccables', 'PSP', 'Ken', 'Équateur', 'Bourbons', 'auxiliaires', 'liturgique', 'Ramgoolam', 'paie', 'Sagittaire', 'decumbens', 'sensibles', 'Sincèrement', 'those']\n",
            "\n",
            "A few items at the end of the vocabulary list\n",
            "['attitude', 'Baptême', 'restantes', 'techniciens', 'Antonio', 'Capra', 'GMT', 'fausses', 'Min', 'protège', 'Insee', '-je', 'stock', 'Villemario', 'Goulag', 'regorge', 'aérage', 'Caïn', 'Schleswig-Holstein', 'Cyangugu', 'Sifuentes', 'aie', 'End', 'joie', 'démonstration', 'Boycott', 'éditées', 'posent', 'bouilli', 'gaspillages', 'transformations', 'cristalline', 'pack', 'facto', 'pouvaient', 'personnelle', 'remarquée', 'Production', 'démontrables', 'râle', 'récent', 'Guennadi', 'Farrell', 'Guines', '1882', 'Easy', 'relooking', 'Figures', 'Lochranza', 'Yisrulik']\n"
          ]
        }
      ],
      "source": [
        "print(\"Vocabulaire:\")\n",
        "print(\"A few items of the vocabulary list\")\n",
        "print(voc_l[0:50])\n",
        "print()\n",
        "print(\"A few items at the end of the vocabulary list\")\n",
        "print(voc_l[-50:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bed4f217",
      "metadata": {
        "id": "bed4f217"
      },
      "source": [
        "\n",
        "\n",
        "Lire toutes les lignes du fichier train_pos en tant que liste de chaînes de caractères. \n",
        "Chaque élément de la liste correspond à une ligne du fichier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4e145582",
      "metadata": {
        "id": "4e145582"
      },
      "outputs": [],
      "source": [
        "# load in the training corpus\n",
        "with open(\"train_pos.txt\", 'r', encoding='utf-8') as f:\n",
        "    training_corpus = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "adc22b05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adc22b05",
        "outputId": "0550c54e-6ac9-426a-b177-231cb2da97db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A few items of the training corpus list\n",
            "['Les\\tDET\\n', 'commotions\\tNFP\\n', 'cérébrales\\tADJFP\\n', 'sont\\tAUX\\n', 'devenu\\tVPPMS\\n']\n"
          ]
        }
      ],
      "source": [
        "print(f\"A few items of the training corpus list\")\n",
        "print(training_corpus[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16dbb90a",
      "metadata": {
        "id": "16dbb90a"
      },
      "source": [
        "Créer un dictionnaire (vocab) qui mappe chaque mot à un indice unique.\n",
        "la clé est le mot et la valeur c'est son unique indice.\n",
        "Il ajoute également une entrée \"--unk--\" pour représenter les mots inconnus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "284ca305",
      "metadata": {
        "id": "284ca305"
      },
      "outputs": [],
      "source": [
        "# vocab: dictionary that has the index of the corresponding words\n",
        "vocab = {} \n",
        "\n",
        "# Get the index of the corresponding words. \n",
        "for i, word in enumerate(sorted(voc_l)): \n",
        "    vocab[word] = i       \n",
        "vocab['--unk--'] = len(vocab)    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "978b9f21",
      "metadata": {
        "id": "978b9f21"
      },
      "source": [
        "Compter le nombre d'apparitions de chaque mot dans une liste de phrases tokenisées et de retourner un dictionnaire qui mappe chaque mot à sa fréquence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c93b4fb4",
      "metadata": {
        "id": "c93b4fb4"
      },
      "outputs": [],
      "source": [
        "def count_words(tokenized_sentences):\n",
        "        \n",
        "    word_counts = {}\n",
        "    \n",
        "    for sentence in tokenized_sentences: \n",
        "        for token in sentence: \n",
        "            if token not in word_counts.keys(): \n",
        "                word_counts[token] = 1\n",
        "            \n",
        "            else:\n",
        "                word_counts[token] += 1    \n",
        "    return word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a85c2a1c",
      "metadata": {
        "id": "a85c2a1c"
      },
      "outputs": [],
      "source": [
        "# load in the test corpus\n",
        "with open(\"test_pos.txt\", 'r', encoding='utf-8') as f:\n",
        "    y = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bd88318d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd88318d",
        "outputId": "705afb4e-3d23-43e2-8c4f-20cd35116304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A sample of the test corpus\n",
            "['Je\\tPPER1S\\n', 'sens\\tVERB\\n', \"qu'\\tCOSUB\\n\", 'entre\\tPREP\\n', 'ça\\tPDEMMS\\n', 'et\\tCOCO\\n', 'les\\tDET\\n', 'films\\tNMP\\n', 'de\\tPREP\\n', 'médecins\\tNMP\\n']\n"
          ]
        }
      ],
      "source": [
        "print(\"A sample of the test corpus\")\n",
        "print(y[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "85a0c455",
      "metadata": {
        "id": "85a0c455"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "# Caractères de ponctuation\n",
        "punct = set(string.punctuation)\n",
        "\n",
        "# Règles de morphologie utilisées pour attribuer des jetons aux mots inconnus\n",
        "noun_suffix = [\"age\", \"al\", \"eau\", \"ée\", \"euse\", \"ie\", \"ière\", \"ion\", \"ité\", \"oire\", \"sme\", \"té\", \"tion\"]\n",
        "verb_suffix = [\"er\", \"ir\", \"oir\", \"re\"]\n",
        "adj_suffix = [\"able\", \"aire\", \"al\", \"e\", \"el\", \"esque\", \"et\", \"i\", \"ien\", \"ier\", \"if\", \"in\", \"ique\", \"iste\", \"itif\", \"ive\", \"oire\", \"ol\", \"on\", \"ot\", \"t\", \"é\"]\n",
        "adv_suffix = [\"amment\", \"emment\", \"iment\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7763f31c",
      "metadata": {
        "id": "7763f31c"
      },
      "source": [
        "Extraire uniquement les mots de chaque ligne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4a914109",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a914109",
        "outputId": "b60605d0-b8ff-4f32-e8c2-cb176658596a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Je', 'sens', \"qu'\", 'entre', 'ça', 'et', 'les', 'films', 'de', 'médecins', 'et', 'scientifiques', 'fous', 'que', 'nous', 'avons', 'déjà', 'vus', ',', 'nous']\n"
          ]
        }
      ],
      "source": [
        "with open('test_pos.txt', 'r', encoding='utf-8') as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Diviser le texte en lignes\n",
        "lines = data.split('\\n')\n",
        "\n",
        "# Extraire uniquement les mots de chaque ligne\n",
        "prep = []\n",
        "for line in lines:\n",
        "    tokens = line.split('\\t')\n",
        "    if len(tokens) > 1:\n",
        "        word = tokens[0]\n",
        "        prep.extend([word])\n",
        "\n",
        "# Afficher la liste des mots extraits\n",
        "print(prep[0:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15d213bc",
      "metadata": {
        "id": "15d213bc"
      },
      "source": [
        "Remplacer les mots de l'ensemble des donnees de teste qui ne font pas partie du vocabulaire donné par le symbole '<--unk-->' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "07da9585",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07da9585",
        "outputId": "6553e9e7-80b1-4cdc-9641-46d1665c1ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Je', 'sens', \"qu'\", 'entre', 'ça', 'et', 'les', 'films', 'de', 'médecins', 'et', 'scientifiques', 'fous', 'que', 'nous', 'avons', 'déjà', 'vus', ',', 'nous']\n"
          ]
        }
      ],
      "source": [
        "def assign_unk(word):\n",
        "    return '--unk--' if word not in vocab.keys() else word\n",
        "prep = [assign_unk(word) for word in prep]\n",
        "print(prep[0:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62772c56",
      "metadata": {
        "id": "62772c56"
      },
      "source": [
        "- Vérifier si la ligne est vide,si c'est le cas, il n'y a pas de mot ou d'étiquette associée. Dans ce cas, on attribue les valeurs --n-- au mot et --s-- l'étiquette.\n",
        "- Vérifier ensuite si la longueur de split_line est différente de 2. Si c'est le cas, cela signifie que la ligne ne contient pas à la fois un mot et une étiquette valide. Dans ce cas, on attribue les valeurs --n-- au mot et --s-- à l'étiquette, et les renvoie.\n",
        "- Si la longueur de split_line est égale à 2, cela signifie que la ligne contient à la fois un mot et une étiquette valide. Elle assigne le mot et l'étiquette aux variables word et tag, respectivement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ba2f5489",
      "metadata": {
        "id": "ba2f5489"
      },
      "outputs": [],
      "source": [
        "def get_word_tag(line, vocab):\n",
        "    if not line.split():\n",
        "        word = \"--n--\"\n",
        "        tag = \"--s--\"\n",
        "        return word, tag\n",
        "    else:\n",
        "        split_line = line.split()\n",
        "        if len(split_line) != 2:\n",
        "            word = \"--n--\"\n",
        "            tag = \"--s--\"\n",
        "        else:\n",
        "            word, tag = split_line\n",
        "            if word not in vocab:\n",
        "                # Gérer les mots inconnus\n",
        "                word = assign_unk(word)\n",
        "        return word, tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b777e3aa",
      "metadata": {
        "id": "b777e3aa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a748fe8",
      "metadata": {
        "id": "8a748fe8"
      },
      "source": [
        "- La foction prend un corpus d'entraînement et un vocabulaire en entrée, et renvoie trois dictionnaires : emission_counts, transition_counts et tag_counts.\n",
        "- Initialiser la variable prev_tag avec l'état de départ, représenté par --s-- .Cela représente l'état précédent dans la séquence de tags.\n",
        "- Vérifier si i est un multiple de 50 000. Si c'est le cas, on affiche le nombre de mots traités jusqu'à présent.\n",
        "- On appelle la fonction get_word_tag pour gèrer les mots inconnus et attribuer des valeurs spéciales.\n",
        "- emission_counts: un dictionnaire pour calculer la proba sachant un tag donne.\n",
        "- transition_counts : un dictionnaire pour calculer le nombre de fois un tag donne est apparue a cote d'un autre tag donne.\n",
        "- tag_counts : le nobre de fois qu'un tag est apparue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b4833958",
      "metadata": {
        "id": "b4833958"
      },
      "outputs": [],
      "source": [
        "def create_dictionaries(training_corpus, vocab):\n",
        "\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "    \n",
        "    prev_tag = '--s--' \n",
        "    i = 0 \n",
        "    \n",
        "    for word_tag in training_corpus:\n",
        "        i += 1\n",
        "        #signifie que 50000 mots ont été traités depuis le dernier affichage de compte.\n",
        "        if i % 50000 == 0:\n",
        "            print(f\"word count = {i}\")\n",
        "            \n",
        "        word, tag = get_word_tag(word_tag,vocab) \n",
        "        \n",
        "        transition_counts[(prev_tag, tag)] += 1\n",
        "        \n",
        "        emission_counts[(tag, word)] += 1\n",
        "\n",
        "        tag_counts[tag] += 1\n",
        "\n",
        "        prev_tag = tag\n",
        "        \n",
        "        \n",
        "    return emission_counts, transition_counts, tag_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "92e2430f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92e2430f",
        "outputId": "546386cc-5839-4f66-8469-94c2bfe0d1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word count = 50000\n",
            "word count = 100000\n",
            "word count = 150000\n",
            "word count = 200000\n",
            "word count = 250000\n",
            "word count = 300000\n",
            "word count = 350000\n"
          ]
        }
      ],
      "source": [
        "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb56d6b1",
      "metadata": {
        "id": "bb56d6b1"
      },
      "source": [
        "- Tout les tags :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e9897540",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9897540",
        "outputId": "2e23cd0a-c794-4807-e855-6f5c4715a5de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of POS tags (number of 'states'): 67\n",
            "View these POS tags (states)\n",
            "['--s--', 'ADJ', 'ADJFP', 'ADJFS', 'ADJMP', 'ADJMS', 'ADV', 'AUX', 'CHIF', 'COCO', 'COSUB', 'DET', 'DETFS', 'DETMS', 'DINTFS', 'DINTMS', 'INTJ', 'MOTINC', 'NFP', 'NFS', 'NMP', 'NMS', 'NOUN', 'NUM', 'PART', 'PDEMFP', 'PDEMFS', 'PDEMMP', 'PDEMMS', 'PINDFP', 'PINDFS', 'PINDMP', 'PINDMS', 'PINTFS', 'PPER1S', 'PPER2S', 'PPER3FP', 'PPER3FS', 'PPER3MP', 'PPER3MS', 'PPOBJFP', 'PPOBJFS', 'PPOBJMP', 'PPOBJMS', 'PREF', 'PREFP', 'PREFS', 'PREL', 'PRELFP', 'PRELFS', 'PRELMP', 'PRELMS', 'PREP', 'PRON', 'PROPN', 'PUNCT', 'SYM', 'VERB', 'VPPFP', 'VPPFS', 'VPPMP', 'VPPMS', 'VPPRE', 'X', 'XFAMIL', 'YPFOR', '_']\n"
          ]
        }
      ],
      "source": [
        "states = sorted(tag_counts.keys())\n",
        "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
        "print(\"View these POS tags (states)\")\n",
        "print(states)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bacbda79",
      "metadata": {
        "id": "bacbda79"
      },
      "source": [
        "- Quelques exemples des dictionnaires:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "81d78b9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81d78b9f",
        "outputId": "da93747d-79bf-483d-bceb-5da99641b9f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transition examples: \n",
            "(('--s--', 'DET'), 1)\n",
            "(('DET', 'NFP'), 4162)\n",
            "(('NFP', 'ADJFP'), 1452)\n",
            "\n",
            "emission examples: \n",
            "(('ADJMP', 'autres'), 135)\n",
            "(('ADJMP', 'petits'), 29)\n",
            "(('NMP', 'fruits'), 9)\n",
            "\n",
            "ambiguous word example: \n",
            "('ADJMS', 'vert') 11\n",
            "('NMS', 'vert') 2\n"
          ]
        }
      ],
      "source": [
        "print(\"transition examples: \")\n",
        "for ex in list(transition_counts.items())[:3]:\n",
        "    print(ex)\n",
        "print()\n",
        "\n",
        "print(\"emission examples: \")\n",
        "for ex in list(emission_counts.items())[200:203]:\n",
        "    print (ex)\n",
        "print()\n",
        "\n",
        "print(\"ambiguous word example: \")\n",
        "for tup,cnt in emission_counts.items():\n",
        "    if tup[1] == 'vert': print (tup, cnt) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07a7385a",
      "metadata": {
        "id": "07a7385a"
      },
      "source": [
        "# Phase de test :"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e61c00b6",
      "metadata": {
        "id": "e61c00b6"
      },
      "source": [
        "Cette fonction utilise emission_counts et lensemble des tags pour prédire les POS des mots dans le corpus de prétraitement (prep). Elle compare ensuite ces prédictions aux étiquettes de référence pour calculer la précision du modèle de prédiction de POS.(accuracy)\n",
        "matrice d emission est proba d'un mot observable sachant qu'on est dans tag donné\n",
        "matrice de transition est la proba des tags sachant son contexte ( avant et lui mm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "145b6e32",
      "metadata": {
        "id": "145b6e32"
      },
      "outputs": [],
      "source": [
        "def predict_pos(prep, y, emission_counts, vocab, states):\n",
        "\n",
        "    num_correct = 0    \n",
        "    all_words = set(emission_counts.keys())\n",
        "    \n",
        "    total = len(y)\n",
        "    for word, y_tup in zip(prep, y): \n",
        "\n",
        "        y_tup_l = y_tup.split()        \n",
        "        # Verify that y_tup contain both word and POS\n",
        "        if len(y_tup_l) == 2:            \n",
        "            # Set the true POS label for this word\n",
        "            true_label = y_tup_l[1]\n",
        "\n",
        "        else:\n",
        "            # If the y_tup didn't contain word and POS, go to next word\n",
        "            continue\n",
        "    \n",
        "        count_final = 0\n",
        "        pos_final = ''\n",
        "        \n",
        "        # If the word is in the vocabulary...\n",
        "        if word in vocab:\n",
        "            for pos in states:\n",
        "\n",
        "            ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "                        \n",
        "                # define the key as the tuple containing the POS and word\n",
        "                key = (pos,word)\n",
        "\n",
        "                # check if the (pos, word) key exists in the emission_counts dictionary\n",
        "                if key in emission_counts: # complete this line\n",
        "\n",
        "                # get the emission count of the (pos,word) tuple \n",
        "                    count = emission_counts[key]\n",
        "\n",
        "                    # keep track of the POS with the largest count\n",
        "                    if count>count_final: # complete this line\n",
        "\n",
        "                        # update the final count (largest count)\n",
        "                        count_final = count\n",
        "\n",
        "                        # update the final POS\n",
        "                        pos_final = pos\n",
        "\n",
        "            # If the final POS (with the largest count) matches the true POS:\n",
        "            if pos_final == true_label: # complete this line\n",
        "                \n",
        "                # Update the number of correct predictions\n",
        "                num_correct += 1\n",
        "            \n",
        "    ### END CODE HERE ###\n",
        "    accuracy = num_correct / total\n",
        "    \n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5fcde386",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fcde386",
        "outputId": "0253415a-81c7-4ec1-b8f4-ecb50fbf45c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of prediction using predict_pos is 0.8775\n"
          ]
        }
      ],
      "source": [
        "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
        "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1e9d66f",
      "metadata": {
        "id": "a1e9d66f"
      },
      "source": [
        "- Hidden Markov Models for POS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "192e5915",
      "metadata": {
        "id": "192e5915"
      },
      "source": [
        "- On utilise tag_counts et transition_counts pour créer une matrice de transition utilisée dans le modèle HMM pour prédire les étiquettes POS des mots. \n",
        "- La lissage est appliquée pour gérer les transitions qui n'ont pas été observées dans les données d'entraînement.(pour eviter les valeurs nulles)\n",
        "- La matrice de transition regroupe les probas de transition entre les differents etats, la somme de chaue ligne egale a 1.\n",
        "- les colonne et les lignes de cette matrice sont les tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9068254a",
      "metadata": {
        "id": "9068254a"
      },
      "outputs": [],
      "source": [
        "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: create_transition_matrix\n",
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
        "    ''' \n",
        "    Input: \n",
        "        alpha: number used for smoothing\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        transition_counts: transition count for the previous word and tag\n",
        "    Output:\n",
        "        A: matrix of dimension (num_tags,num_tags)\n",
        "    '''\n",
        "    # Get a sorted list of unique POS tags\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    \n",
        "    # Count the number of unique POS tags\n",
        "    num_tags = len(all_tags)\n",
        "    \n",
        "    # Initialize the transition matrix 'A'\n",
        "    A = np.zeros((num_tags,num_tags))\n",
        "    \n",
        "    # Get the unique transition tuples (previous POS, current POS)\n",
        "    trans_keys = set(transition_counts.keys())\n",
        "    \n",
        "    ### START CODE HERE (Return instances of 'None' with your code) ### \n",
        "    \n",
        "    # Go through each row of the transition matrix A\n",
        "    for i in range(num_tags):\n",
        "        \n",
        "        # Go through each column of the transition matrix A\n",
        "        for j in range(num_tags):\n",
        "\n",
        "            # Initialize the count of the (prev POS, current POS) to zero\n",
        "            count = 0\n",
        "        \n",
        "            # Define the tuple (prev POS, current POS)\n",
        "            # Get the tag at position i and tag at position j (from the all_tags list)\n",
        "            key = (all_tags[i],all_tags[j])\n",
        "\n",
        "            # Check if the (prev POS, current POS) tuple \n",
        "            # exists in the transition counts dictionaory\n",
        "            if transition_counts: #complete this line\n",
        "                \n",
        "                # Get count from the transition_counts dictionary \n",
        "                # for the (prev POS, current POS) tuple\n",
        "                count = transition_counts[key]\n",
        "                \n",
        "            # Get the count of the previous tag (index position i) from tag_counts\n",
        "            count_prev_tag = tag_counts[all_tags[i]]\n",
        "            \n",
        "            # Apply smoothing using count of the tuple, alpha, \n",
        "            # count of previous tag, alpha, and number of total tags\n",
        "            A[i,j] = (count + alpha) / (count_prev_tag + alpha*num_tags)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "febbda22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "febbda22",
        "outputId": "e4c6c8ba-e51b-409c-84be-9a710ed144a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A at row 0, col 0: 0.000006491\n",
            "A at row 3, col 1: 0.0003\n",
            "View a subset of transition matrix A\n",
            "          PINDFS    PINDMP    PINDMS    PINTFS    PPER1S\n",
            "PINDFS  0.000008  0.000008  0.000008  0.000008  0.000008\n",
            "PINDMP  0.000012  0.000012  0.000012  0.000012  0.000012\n",
            "PINDMS  0.000001  0.000001  0.000001  0.000001  0.000001\n",
            "PINTFS  0.000484  0.000484  0.000484  0.000484  0.000484\n",
            "PPER1S  0.000002  0.000002  0.000002  0.000002  0.000002\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.001\n",
        "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
        "# Testing your function\n",
        "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
        "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
        "\n",
        "print(\"View a subset of transition matrix A\")\n",
        "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
        "print(A_sub)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "937757ce",
      "metadata": {
        "id": "937757ce"
      },
      "source": [
        "- la matrice d'emission regroupe les proba des mots sachant le tags.\n",
        "- les colonnes sont representes par les mots et les lignes sont representes par les tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e50343bb",
      "metadata": {
        "id": "e50343bb"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: create_emission_matrix\n",
        "\n",
        "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        alpha: tuning parameter used in smoothing \n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "    Output:\n",
        "        B: a matrix of dimension (num_tags, len(vocab))\n",
        "    '''\n",
        "    \n",
        "    # get the number of POS tag\n",
        "    num_tags = len(tag_counts)\n",
        "    \n",
        "    # Get a list of all POS tags\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    \n",
        "    # Get the total number of unique words in the vocabulary\n",
        "    num_words = len(vocab)\n",
        "    \n",
        "    # Initialize the emission matrix B with places for\n",
        "    # tags in the rows and words in the columns\n",
        "    B = np.zeros((num_tags, num_words))\n",
        "    \n",
        "    # Get a set of all (POS, word) tuples \n",
        "    # from the keys of the emission_counts dictionary\n",
        "    emis_keys = set(list(emission_counts.keys()))\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    \n",
        "    # Go through each row (POS tags)\n",
        "    for i in range(num_tags): # complete this line\n",
        "        \n",
        "        # Go through each column (words)\n",
        "        for j in range(num_words): # complete this line\n",
        "\n",
        "            # Initialize the emission count for the (POS tag, word) to zero\n",
        "            count = 0\n",
        "                    \n",
        "            # Define the (POS tag, word) tuple for this row and column\n",
        "            key =  (all_tags[i],vocab[j])\n",
        "\n",
        "            # check if the (POS tag, word) tuple exists as a key in emission counts\n",
        "            if key in emission_counts.keys(): # complete this line\n",
        "        \n",
        "                # Get the count of (POS tag, word) from the emission_counts d\n",
        "                count = emission_counts[key]\n",
        "                \n",
        "            # Get the count of the POS tag\n",
        "            count_tag = tag_counts[all_tags[i]]\n",
        "                \n",
        "            # Apply smoothing and store the smoothed value \n",
        "            # into the emission matrix B for this row and column\n",
        "            B[i,j] = (count + alpha) / (count_tag+ alpha*num_words)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1d25e7bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d25e7bc",
        "outputId": "bb4849db-e005-4fa4-de1f-1996d975d939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View Matrix position at row 0, column 0: 0.000005093\n",
            "View Matrix position at row 3, column 1: 0.000000140\n",
            "                  1      médecins            et  scientifiques          fous\n",
            "ADJ    1.005692e-06  1.005692e-06  1.005692e-06   1.005692e-06  1.005692e-06\n",
            "ADJMP  3.398655e-07  3.398655e-07  3.398655e-07   3.402054e-04  3.398655e-07\n",
            "ADJMS  1.358633e-07  1.358633e-07  1.358633e-07   1.358633e-07  1.358633e-07\n",
            "ADV    7.575562e-08  7.575562e-08  7.575562e-08   7.575562e-08  7.575562e-08\n",
            "AUX    8.734128e-08  8.734128e-08  8.734128e-08   8.734128e-08  8.734128e-08\n",
            "CHIF   9.857355e-03  1.095249e-07  1.095249e-07   1.095249e-07  1.095249e-07\n"
          ]
        }
      ],
      "source": [
        "# creating your emission probability matrix. this takes a few minutes to run. \n",
        "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
        "\n",
        "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
        "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
        "\n",
        "# Try viewing emissions for a few words in a sample dataframe\n",
        "cidx  = ['1','médecins', 'et', 'scientifiques', 'fous']\n",
        "\n",
        "# Get the integer ID for each word\n",
        "cols = [vocab[a] for a in cidx]\n",
        "\n",
        "# Choose POS tags to show in a sample dataframe\n",
        "rvals =['ADJ', 'ADJMP', 'ADJMS', 'ADV', 'AUX', 'CHIF',]\n",
        "\n",
        "# For each POS tag, get the row number from the 'states' list\n",
        "rows = [states.index(a) for a in rvals]\n",
        "\n",
        "# Get the emissions for the sample of words, and the sample of POS tags\n",
        "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
        "print(B_sub)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52331650",
      "metadata": {
        "id": "52331650"
      },
      "source": [
        "# Viterbi Algorithm and Dynamic Programming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "3c479d0f",
      "metadata": {
        "id": "3c479d0f"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: initialize\n",
        "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        states: a list of all possible parts-of-speech\n",
        "        tag_counts: a dictionary mapping each tag to its respective count\n",
        "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
        "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
        "        corpus: a sequence of words whose POS is to be identified in a list \n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
        "    Output:\n",
        "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
        "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
        "    '''\n",
        "    # Get the total number of unique POS tags\n",
        "    num_tags = len(tag_counts)\n",
        "    \n",
        "    # Initialize best_probs matrix \n",
        "    # POS tags in the rows, number of words in the corpus as the columns\n",
        "    best_probs = np.zeros((num_tags, len(corpus)))\n",
        "    \n",
        "    # Initialize best_paths matrix\n",
        "    # POS tags in the rows, number of words in the corpus as columns\n",
        "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
        "    \n",
        "    # Define the start token\n",
        "    s_idx = states.index(\"--s--\")\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    \n",
        "    # Go through each of the POS tags\n",
        "    for i in range(num_tags): # complete this line\n",
        "        \n",
        "        # Handle the special case when the transition from start token to POS tag i is zero\n",
        "        if A[s_idx,i] == 0: # complete this line\n",
        "            \n",
        "            # Initialize best_probs at POS tag 'i', column 0, to negative infinity\n",
        "            best_probs[i,0] = float('-inf')\n",
        "        \n",
        "        # For all other cases when transition from start token to POS tag i is non-zero:\n",
        "        else:\n",
        "            \n",
        "            # Initialize best_probs at POS tag 'i', column 0\n",
        "            # Check the formula in the instructions above\n",
        "            best_probs[i,0] = math.log(A[s_idx,i]) + math.log(B[i,vocab[corpus[0]]] )\n",
        "                        \n",
        "    ### END CODE HERE ### \n",
        "    return best_probs, best_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e1e653ff",
      "metadata": {
        "id": "e1e653ff"
      },
      "outputs": [],
      "source": [
        "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "8da3a552",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8da3a552",
        "outputId": "06d5587a-26c6-4d8b-8ddc-57bfb7acf104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_probs[0,0]: -24.1327\n",
            "best_paths[2,3]: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Test the function\n",
        "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n",
        "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "8fcf63fa",
      "metadata": {
        "id": "8fcf63fa"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: viterbi_forward\n",
        "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
        "    '''\n",
        "    Input: \n",
        "        A, B: The transiton and emission matrices respectively\n",
        "        test_corpus: a list containing a preprocessed corpus\n",
        "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
        "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
        "    Output: \n",
        "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
        "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
        "    '''\n",
        "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
        "    num_tags = best_probs.shape[0]\n",
        "    \n",
        "    # Go through every word in the corpus starting from word 1\n",
        "    # Recall that word 0 was initialized in `initialize()`\n",
        "    for i in range(1, len(test_corpus)): \n",
        "        \n",
        "        # Print number of words processed, every 5000 words\n",
        "        if i % 5000 == 0:\n",
        "            print(\"Words processed: {:>8}\".format(i))\n",
        "            \n",
        "        ### START CODE HERE (Replace instances of 'None' with your code EXCEPT the first 'best_path_i = None') ###\n",
        "        # For each unique POS tag that the current word can be\n",
        "        for j in range(num_tags): # complete this line\n",
        "            \n",
        "            # Initialize best_prob for word i to negative infinity\n",
        "            best_prob_i = float(\"-inf\")\n",
        "            \n",
        "            # Initialize best_path for current word i to None\n",
        "            best_path_i = None\n",
        "\n",
        "            # For each POS tag that the previous word can be:\n",
        "            for k in range(num_tags): # complete this line\n",
        "            \n",
        "                # Calculate the probability = \n",
        "                # best probs of POS tag k, previous word i-1 + \n",
        "                # log(prob of transition from POS k to POS j) + \n",
        "                # log(prob that emission of POS j is word i)\n",
        "                prob = best_probs[k,i-1]+math.log(A[k,j]) +math.log(B[j,vocab[test_corpus[i]]])\n",
        "\n",
        "                # check if this path's probability is greater than\n",
        "                # the best probability up to and before this point\n",
        "                if prob > best_prob_i: # complete this line\n",
        "                    \n",
        "                    # Keep track of the best probability\n",
        "                    best_prob_i = prob\n",
        "                    \n",
        "                    # keep track of the POS tag of the previous word\n",
        "                    # that is part of the best path.  \n",
        "                    # Save the index (integer) associated with \n",
        "                    # that previous word's POS tag\n",
        "                    best_path_i = k\n",
        "\n",
        "            # Save the best probability for the \n",
        "            # given current word's POS tag\n",
        "            # and the position of the current word inside the corpus\n",
        "            best_probs[j,i] = best_prob_i\n",
        "            \n",
        "            # Save the unique integer ID of the previous POS tag\n",
        "            # into best_paths matrix, for the POS tag of the current word\n",
        "            # and the position of the current word inside the corpus.\n",
        "            best_paths[j,i] = best_path_i\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "    return best_probs, best_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a1e5483c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1e5483c",
        "outputId": "46c28bed-26f4-4c4d-9bad-c3b49607f6e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words processed:     5000\n",
            "Words processed:    10000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d1815094",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1815094",
        "outputId": "e1a207dc-d2b0-47ad-9811-90ebec0a2e9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_probs[0,1]: -33.9225\n",
            "best_probs[0,4]: -55.6327\n"
          ]
        }
      ],
      "source": [
        "# Test this function \n",
        "print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n",
        "print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8b37d93e",
      "metadata": {
        "id": "8b37d93e"
      },
      "outputs": [],
      "source": [
        "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: viterbi_backward\n",
        "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
        "    '''\n",
        "    This function returns the best path.\n",
        "    \n",
        "    '''\n",
        "    # Get the number of words in the corpus\n",
        "    # which is also the number of columns in best_probs, best_paths\n",
        "    m = best_paths.shape[1] \n",
        "    \n",
        "    # Initialize array z, same length as the corpus\n",
        "    z = [None] * m\n",
        "    \n",
        "    # Get the number of unique POS tags\n",
        "    num_tags = best_probs.shape[0]\n",
        "    \n",
        "    # Initialize the best probability for the last word\n",
        "    best_prob_for_last_word = float('-inf')\n",
        "    \n",
        "    # Initialize pred array, same length as corpus\n",
        "    pred = [None] * m\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    ## Step 1 ##\n",
        "    \n",
        "    # Go through each POS tag for the last word (last column of best_probs)\n",
        "    # in order to find the row (POS tag integer ID) \n",
        "    # with highest probability for the last word\n",
        "    for k in range(num_tags): # complete this line\n",
        "\n",
        "        # If the probability of POS tag at row k \n",
        "        # is better than the previosly best probability for the last word:\n",
        "        if best_probs[k,-1]>best_prob_for_last_word: # complete this line\n",
        "            \n",
        "            # Store the new best probability for the lsat word\n",
        "            best_prob_for_last_word = best_probs[k,-1]\n",
        "    \n",
        "            # Store the unique integer ID of the POS tag\n",
        "            # which is also the row number in best_probs\n",
        "            z[m - 1] = k\n",
        "            \n",
        "    # Convert the last word's predicted POS tag\n",
        "    # from its unique integer ID into the string representation\n",
        "    # using the 'states' dictionary\n",
        "    # store this in the 'pred' array for the last word\n",
        "    pred[m - 1] = states[k]\n",
        "    \n",
        "    ## Step 2 ##\n",
        "    # Find the best POS tags by walking backward through the best_paths\n",
        "    # From the last word in the corpus to the 0th word in the corpus\n",
        "    for i in range(len(corpus)-1, -1, -1): # complete this line\n",
        "        \n",
        "        # Retrieve the unique integer ID of\n",
        "        # the POS tag for the word at position 'i' in the corpus\n",
        "        pos_tag_for_word_i = best_paths[np.argmax(best_probs[:,i]),i]\n",
        "        \n",
        "        # In best_paths, go to the row representing the POS tag of word i\n",
        "        # and the column representing the word's position in the corpus\n",
        "        # to retrieve the predicted POS for the word at position i-1 in the corpus\n",
        "        z[i - 1] = best_paths[pos_tag_for_word_i,i]\n",
        "        \n",
        "        # Get the previous word's POS tag in string form\n",
        "        # Use the 'states' dictionary, \n",
        "        # where the key is the unique integer ID of the POS tag,\n",
        "        # and the value is the string representation of that POS tag\n",
        "        pred[i - 1] = states[pos_tag_for_word_i]\n",
        "        \n",
        "     ### END CODE HERE ###\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "de8473ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de8473ce",
        "outputId": "14c9bca4-fa8b-406b-922c-6225831c09b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction for pred[-7:m-1] is: \n",
            " ['Nancy', ',', 'est', 'une', '--unk--', 'française'] \n",
            " ['PROPN', 'PUNCT', 'AUX', 'DINTFS', 'NFS', 'ADJFS'] \n",
            "\n",
            "The prediction for pred[0:8] is: \n",
            " ['PPER1S', 'VERB', 'COSUB', 'PREP', 'PDEMMS', 'COCO', 'DET'] \n",
            " ['Je', 'sens', \"qu'\", 'entre', 'ça', 'et', 'les']\n"
          ]
        }
      ],
      "source": [
        "# Run and test your function\n",
        "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
        "m=len(pred)\n",
        "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
        "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "53ce7e4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53ce7e4a",
        "outputId": "5ea445a4-363a-45db-a45e-4c7853cb191a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The third word is: entre\n",
            "Your prediction is: PREP\n",
            "Your corresponding label y is:  entre\tPREP\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('The third word is:', prep[3])\n",
        "print('Your prediction is:', pred[3])\n",
        "print('Your corresponding label y is: ', y[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "911baaf7",
      "metadata": {
        "id": "911baaf7"
      },
      "outputs": [],
      "source": [
        "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: compute_accuracy\n",
        "def compute_accuracy(pred, y):\n",
        "    '''\n",
        "    Input: \n",
        "        pred: a list of the predicted parts-of-speech \n",
        "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
        "    Output: \n",
        "        \n",
        "    '''\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Zip together the prediction and the labels\n",
        "    for prediction, y in zip(pred, y):\n",
        "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "        # Split the label into the word and the POS tag\n",
        "        word_tag_tuple = y.split()\n",
        "        \n",
        "        # Check that there is actually a word and a tag\n",
        "        # no more and no less than 2 items\n",
        "        if len(word_tag_tuple)!=2: # complete this line\n",
        "            continue \n",
        "\n",
        "        # store the word and tag separately\n",
        "        word, tag = word_tag_tuple\n",
        "        \n",
        "        # Check if the POS tag label matches the prediction\n",
        "        if prediction == tag: # complete this line\n",
        "            \n",
        "            # count the number of times that the prediction\n",
        "            # and label match\n",
        "            num_correct += 1\n",
        "            \n",
        "        # keep track of the total number of examples (that have valid labels)\n",
        "        total += 1\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "    return num_correct/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "890b7c6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "890b7c6a",
        "outputId": "768dac0b-7a67-49cd-c6b7-b92f9240242d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Viterbi algorithm is 0.9168\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "057a65de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "057a65de",
        "outputId": "f56f4d3f-b965-41be-b6d5-0ae9cab28203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 5 word is: et\n",
            "Your prediction is: COCO\n",
            "Your corresponding label y is:  et\tCOCO\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('The 5 word is:', prep[5])\n",
        "print('Your prediction is:', pred[5])\n",
        "print('Your corresponding label y is: ', y[5])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}